\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish,mexico]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
%\usepackage[left=2cm,right=2cm,top=2cm,bottom=3cm]{geometry}
\author{Miguel Angel Escalante Serrato}
\title{Examen 1 Estadística Computacional}
\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
\section{Pregunta 1}
\subsection{a)}
La probabilidad de ganar se estima como sigue:
<<echo=FALSE>>=
## Preg 1
p <- 18/38
n <- 20
juego <- function(){
    p <- 18/38
    n <- 20
    va <- rbinom(prob=p,size=1,n=n)
    ex <- sum(va)
    return((ex-(n-ex))*5)
}

js <- replicate(10000,juego())
mean(js>0)
@
\subsection{b)}
La ganancia la podemos modelar con una binomial con $n=20,p=\frac{18}{38}$, por lo que al calcular la probabilidad teórica es la que sigue: 
\[1-F_X(k=10,n=20,p=\frac{18}{38})\]
Con $X\sim Bin\left(20,\frac{18}{38}\right)$
<<echo=FALSE>>=
1-pbinom(10,20,18/38)
@
\subsection{c)}
Que podemos ver que se parece bastante a la estimada; por otro lado la tabla de frecuencias de ganancias dado que se tiene ganancia es la que sigue:

<<echo=FALSE>>=
js <- replicate(500,juego())
ganancias <- js[js>0]
table(ganancias)
@
Y podemos visualizarlo en el gráfico de barras de la figura \ref{1a}. 
\begin{figure}[width=10cm]
\centering
<<echo=FALSE,fig=TRUE>>=
barplot(table(ganancias))
@
\caption{Frecuencias por ganancia.}
\label{1a}
\end{figure}
Lo que viene es lo evidente, que es que las ganancias entre más altas son menos probables; más aún, se tiene acotada la ganancia dados los 20 juegos en 200, pero en 500 juegos simulados vemos que no pasa de 50 la ganancia que se tiene; ahora si lo corremos más veces la simulación, empiezan a aparecer valores más altos, pero su probabilidad sigue siendo muy baja:
<<echo=FALSE>>=
js <- replicate(500000,juego())
ganancias <- js[js>0]
table(ganancias)
@
Para este caso simulé 500,000 juegos. 

\section{Pregunta 2}
\subsection{a)}
Determiné que el valor adecuado para $A$, fue el máximo de $f(u)$ con $u\in (0,5)$. en este caso me quedé con 17.
\subsection{b)}
Para ver el código que genera observaciones de la variable aleatoria, ver el anexo de código en R. 
\begin{figure}[width=10cm]
\centering
<<echo=FALSE, fig=true>>=
## Preg 2
f <- function(x){
  xdd <- (x>=0)&(x<=5)
  res <- c()
  res[!xdd] <- 0
  res[xdd] <- x[xdd]^2*(5-x[xdd])*sin(2*x[xdd])^2
  return(res)
}
x <- seq(-2,6,0.001)
plot(x,f(x),type='l')
@
\caption{Gráfico que ilustra la función $f$, pregunta 2.}
\label{Gráfica de la función f}
\end{figure}
Luego, para simular la variable aleatoria $X$, se programa un poco más, y se obtiene la  gráfica de la figura \ref{2c} para estimar la densidad de $X$:
\begin{figure}[width=10cm]
\centering
<<echo=FALSE,fig=TRUE>>=
#summary(f(x))
A <- 17
u <- runif(1000,0,5)
v <- runif(1000,0,A)
u <- u[v<f(u)]
plot(density(u),main="Densidad",xlab="x",ylab="f(x)")
@
\caption{Figura conla función de densidad de probabilidad}
\label{2c}
\end{figure}


\section{Pregunta 3}
\subsection{a)}
La matriz de dispersiones con correlaciones la tenemos en la figura \ref{3a}
\begin{figure}
\centering
<<echo=FALSE,fig=TRUE,results=hide>>=
## Preg 3
library(GGally)
library(bootstrap)
library(boot)
#a)
datos <- scor
ggpairs(scor)
@
\caption{Matriz de correlaciones con dispersiones}
\label{3a}
\end{figure}
\subsection{b)}
Para los intervalos de confianza los sacaré con la librería boot, y tiene un paquete especial que saca los intervalos de confianza de alguna muestra de bootstrap. Los intervalos quedaron como siguen:
<<echo=FALSE>>=
rho12 <- cbind(scor$mec,scor$vec)
rho34 <- cbind(scor$alg,scor$ana)
rho35 <- cbind(scor$alg,scor$sta)
rho45 <- cbind(scor$ana,scor$sta)
rho14 <- cbind(scor$mec,scor$ana)
rho <- list(rho12,rho34,rho35,rho45,rho14)
resultados<-list()
nombres <- c("rho12", "rho34", "rho35", "rho45", "rho14")
for (j in seq(1,5,1)){
  i <- rho[[j]]
  nstrap <- floor(n*log(n)^2)
  theta <- function(datos,x){ cor(datos[x,1],datos[x,2]) }
  resultados <- boot(data=i,statistic=theta,R=500)
  ci <- boot.ci(resultados,conf=0.90,type="norm")
  print(nombres[j])
  print(ci$normal)
}

@

\section{Pregunta 4}
\subsection{a)}
La distribución acumulada, calculada a mano es la que sigue; sólo se ocupa un cambio de variable $u=\left(\frac{x}{\lambda}\right)^k$ y sale directa. 
\[F_X(x)=\begin{cases}
1-e^{-\left(\frac{x}{\lambda}\right)^k}&\text{ Si } x\geq 0\\
0&\text{ En otro caso. }
\end{cases}
\]
\subsection{b)}
<<echo=FALSE>>=
library(stats)
library(msm)
lambda <- 2
k <- 3
fw <- function(x,l,k){
  xdd <- (x>=0)
  res <- c()
  res[!xdd] <- 0
  res[xdd] <- k/l*(x[xdd]/l)^(k-1)*exp(-(x[xdd]/l)^k)
  return(res)
}   
Fw<- function(x,l,k){
  xdd <- (x>=0)
  res <- c()
  res[!xdd] <- 0
  res[xdd] <- 1-exp(-(x[xdd]/l)^k)
  return(res)
}
@
Calculado con la función real, $P(X>4)$ es la que sigue:
<<echo=FALSE>>=
1-Fw(4,lambda,k)
@
Vía simulación la probabilidad obtenida es la siguiente: 
<<echo=FALSE>>=
ma <- rweibull(30000,k,lambda)
mean(ma>4)
xm4<-ma[ma>4]
@
Ahora, para sacar la estimación vía muestreo por importancia, primero estimamos los parámetros $\mu$ y $\sigma^2$; como sigue las muestrales de la muestra weibull; ahora el estimado que nos da para este caso es el que sigue: 
<<echo=FALSE>>=
mu <- mean(ma)
sigmasq <- var(ma)
n<- 30000
sum(rnorm(n, mean=mu, sd=sqrt(sigmasq)) > 4) / n
@
que de nuevo concuerda con los valores anteriores. 
\section{Pregunta 5}
\subsection{a)}
Primero programamos la función para generar la mcmc. Para ver la función ver el código de R. 

\begin{figure}[width=10]
\centering
<<echo=FALSE,results=hide,fig=TRUE>>=
## Preg 5
set.seed(108683)
k0<-321
n0<-600
bin <- function(p,k=k0,n=n0 ){
  choose(n, k)*((p)^k)*(1-p)^(n-k)
}
f <- function(x,a=0,b=1){
  return(rep(1/(b-a),length(x)))
  
}
x<-seq(0,1,0.01)
plot(x,f(x),type='l')
@
\caption{Función uniforme(Flat)}
\label{5af}
\end{figure}
\begin{figure}[width=10]
\centering
<<echo=FALSE,fig=TRUE>>=
j <- function(th,n=n0){
  n/(th*(1-th))
}

plot(x,j(x),type='l')
@
\caption{Función uniforme(Jeffreys)}
\label{5af}
\end{figure}

<<echo=false>>=
# Haré que la función reciba un parámetro extra para decidir la función a usar. 
metrop <- function(x0,n,pr){
  f <- function(x,a=0,b=1){
    if((x<b) & (x>a)){
      1/(b-a)  
    }
  }
  j <- function(th,n=n0){
    n/(th*(1-th))
  }
  if (pr=="j"){
    f <- j
  }
  x <- c()
  k <- 0
  x <- append(x,x0)
  for(i in 2:n){
      y <- runif(1)
      r <- bin(y) * f(x[i-1]) / ((bin(x[i-1])* f(y)))
      u <- runif(1)
      if (u <= r){
        x[i] <- y
      	k <- k
      } else
      {
      	x[i] <- x[i-1]
      	k <- k + 1
      }
	}
  return(list(x, c(k/n,n)))
}
@
\section{c)}
Usando el script \textbf{batchmeans.r} nos quedan los siguientes resultados.

Para la a-priori flat tenemos los siguientes resultados y en la figura \ref{5afm} se puede ver el gráfico de estimación de la media mientras aumenta el tamaño.

<<echo=false>>=
source('batchmeans.r')
fl<-metrop(0.5,10000,'')
x_u<-fl[[1]]
bm(x_u)
@
\begin{figure}[width=10cm]
<<echo=FALSE,fig=true>>=
estvssamp(x_u)
@
\caption{Función uniforme(Jeffreys)}
\label{5afm}
\end{figure}
para la a-priori Jeffreys tenemos los siguientes resultados y en la figura \ref{5afm} se puede ver el gráfico de estimación de la media mientras aumenta el tamaño.

<<echo=FALSE>>=
fj<-metrop(0.5,10000,'j')
x_j<-fj[[1]]
bm(x_j)
@
\begin{figure}[width=10cm]
<<echo=FALSE,fig=TRUE>>=
estvssamp(x_j)
@
\caption{Función uniforme(Jeffreys)}
\label{5afm}
\end{figure}
en ambos casos fueron 10,000 pasos, de los cuales se queda con alrededor de un  94\% de los casos.

\subsection{d)}
Ahora lo que hacemos es modificar la función $bin$ para poder generar los casos que se plantean a continuación en el examen. Para el caso de la función plana, tenemos: 
<<echo=FALSE>>=
k0<-28
n0<-50
bin <- function(p,k=k0,n=n0 ){
  choose(n, k)*((p)^k)*(1-p)^(n-k)
}
fdf<-metrop(0.1,10000,'')
mean(fdf[[1]])
@
ahora para el caso de la función jeffreys:
<<echo=FALSE>>=
fdj<-metrop(0.1,10000,'j')
mean(fdj[[1]])
@
Que son bastante cercanos al valor real.
\subsection{e)}
Para este caso es análogo y vemos que se modifica un poco más la información con incrementar el tamaño de la muestra, esto es, la información recolectada va pesando más que la información previa.
<<echo=FALSE>>=
k0<-280
n0<-500
bin <- function(p,k=k0,n=n0 ){
  choose(n, k)*((p)^k)*(1-p)^(n-k)
}
fdf<-metrop(0.1,10000,'')
mean(fdf[[1]])
@
ahora para el caso de la función jeffreys:
<<echo=FALSE>>=
fdj<-metrop(0.1,10000,'j')
mean(fdj[[1]])
@

\subsection{f)}
En este caso es parte de lo que me gusta mucho de los métodos bayesianos, que es el hecho que no descartan los casos para los que no se tiene evidencia; en este caso a pesar de que se tiene k=0, podemos ver que las estimaciones del velor no son ceros, sólo muy cercanas a cero. en ambas apriori's. Primero la plana:
<<echo=FALSE>>=
k0<-0
n0<-500
bin <- function(p,k=k0,n=n0 ){
  choose(n, k)*((p)^k)*(1-p)^(n-k)
}
fdf<-metrop(0.1,10000,'')
mean(fdf[[1]])
@
ahora para el caso de la función jeffreys:
<<echo=FALSE>>=
fdj<-metrop(0.1,10000,'j')
mean(fdj[[1]])
@

\end{document}
